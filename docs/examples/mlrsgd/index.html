<!DOCTYPE html><html lang="en-us"><head><link rel="shortcut icon" type="image/png" sizes="16x16" href="https://dsc-spidal.github.io/harp-test/img/favicon-16x9.png"><link rel="shortcut icon" type="image/png" sizes="32x32" href="https://dsc-spidal.github.io/harp-test/img/favicon-32x18.png"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Hugo 0.18" /><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="harp"><meta property="og:image" content="/img/logo54x54.png"><meta property="og:url" content="https://dsc-spidal.github.io/harp-test/"><meta name="twitter:title" content="harp"><meta name="twitter:image" content="/img/0-1-1.png"><meta property="og:site_name" content="Harp Multiclass Logistic Regression with Stochastic Gradient Descent"><meta property="og:url" content="/docs/examples/mlrsgd/"><meta property="og:description" content=""><meta name="twitter:description" content=""><title>harp Documentation - Harp Multiclass Logistic Regression with Stochastic Gradient Descent</title><link rel="stylesheet" href="https://dsc-spidal.github.io/harp-test/css/style.min.css"><link rel="stylesheet" href="https://dsc-spidal.github.io/harp-test/css/font-awesome.min.css"><link rel="stylesheet" href="https://dsc-spidal.github.io/harp-test/css/pygments.css"></head><body><nav class="hn-top-navbar navbar navbar-inverse navbar-fixed-top" role="navigation"><div class="hn-navbar-container container-fluid"><div class="navbar-header"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#hn-navbar" aria-controls="hn-navbar" aria-expanded="false"><span class="sr-only"><Toggle>navigation</Toggle></span><i class="hn-toggle-button fa fa-bars"></i></button><a class="hn-navbar-logo navbar-brand" href="https://dsc-spidal.github.io/harp-test/"><img class="pull-left" src="https://dsc-spidal.github.io/harp-test/img/0-1-2.png" width="62%"></a></div><div id="hn-navbar" class="navbar-collapse collapse"><ul class="nav navbar-nav navbar-right"><li><a href="https://dsc-spidal.github.io/harp-test/docs/getting-started">Docs</a></li><li><a href="https://dsc-spidal.github.io/harp-test/api">API</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/resources">Resources</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/contributors/community">Community</a></li><li><a href="https://github.com/DSC-SPIDAL/harp-test/">GitHub</a></li><li><a href="https://groups.google.com/forum/#!forum/harp-users">Mailing List</a></li></ul></div></div></nav><div class="hn-main"><div class="container"><div class="row"><aside class="hn-sidebar hidden-xs col-sm-4 col-md-3 col-lg-2 collapse"><nav class="hn-sidebar-nav"><div id="hn-accordion" class="panel-group" role="tablist" aria-multiselectable="true"><div class="panel panel-default"><section id="quick-start" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-quick-start" aria-labelledby="quick-start"><i class="fa fa-caret-right"></i>Quick Start</a></h4></section><div id="collapse-quick-start" class="panel-collapse collapse" aria-labelledby="quick-start"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/getting-started">Harp Installation (single)</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/getting-started-cluster">Harp Installation (cluster)</a></li></ul></div></div></div><div class="panel panel-default"><section id="programming-guides" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-programming-guides" aria-labelledby="programming-guides"><i class="fa fa-caret-right"></i>Programming Guides</a></h4></section><div id="collapse-programming-guides" class="panel-collapse collapse" aria-labelledby="programming-guides"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/programming/overview">Overview</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/programming/data-interface">Data Interfaces and Types</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/programming/scheduler">Schedulers</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/programming/computation-models">Computation Models</a></li></ul></div></div></div><div class="panel panel-default"><section id="collective-communication" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-collective-communication" aria-labelledby="collective-communication"><i class="fa fa-caret-right"></i>Collective Communication</a></h4></section><div id="collapse-collective-communication" class="panel-collapse collapse" aria-labelledby="collective-communication"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/broadcast">broadcast</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/reduce">reduce</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/allgather">allgather</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/allreduce">allreduce</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/regroup">regroup</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/pushandpull">push &amp; pull</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/communications/rotate">rotate</a></li></ul></div></div></div><div class="panel panel-default"><section id="examples" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-examples" aria-labelledby="examples"><i class="fa fa-caret-right"></i>Examples</a></h4></section><div id="collapse-examples" class="panel-collapse collapse" aria-labelledby="examples"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/overview">Overview</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/kmeans">K-Means</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/mlrsgd">Multiclass Logistic Regression</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/lda">Latent Dirichlet Allocation (CVB)</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/svm">Support Vector Machine</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/rf">Random Forests</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/examples/nn">Neural Network</a></li></ul></div></div></div><div class="panel panel-default"><section id="applications" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-applications" aria-labelledby="applications"><i class="fa fa-caret-right"></i>Applications</a></h4></section><div id="collapse-applications" class="panel-collapse collapse" aria-labelledby="applications"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/applications/lda-cgs">Latent Dirichlet Allocation (CGS)</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/applications/mf">Matrix Factorization</a></li></ul></div></div></div><div class="panel panel-default"><section id="harp-daal" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-harp-daal" aria-labelledby="harp-daal"><i class="fa fa-caret-right"></i>Harp-DAAL</a></h4></section><div id="collapse-harp-daal" class="panel-collapse collapse" aria-labelledby="harp-daal"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/harpdaal/harpdaal">Harp-DAAL</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/harpdaal/mfsgd">Matrix Factorization (SGD)</a></li></ul></div></div></div><div class="panel panel-default"><section id="harp-resources" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-harp-resources" aria-labelledby="harp-resources"><i class="fa fa-caret-right"></i>Harp Resources</a></h4></section><div id="collapse-harp-resources" class="panel-collapse collapse" aria-labelledby="harp-resources"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/resources">Harp Resources</a></li></ul></div></div></div><div class="panel panel-default"><section id="contributors" class="panel-heading" role="tab"><h4 class="panel-title"><a role="button" data-toggle="collapse" data-parent="#hn-accordion" href="#collapse-contributors" aria-labelledby="contributors"><i class="fa fa-caret-right"></i>Contributors</a></h4></section><div id="collapse-contributors" class="panel-collapse collapse" aria-labelledby="contributors"><div class="panel-body"><ul><li><a href="https://dsc-spidal.github.io/harp-test/docs/contributors/community">Community</a></li><li><a href="https://dsc-spidal.github.io/harp-test/docs/contributors/contributors">Contributors</a></li></ul></div></div></div></div></nav></aside><section class="hn-docs-main col-sm-8 col-md-9 col-lg-10 col-sm-offset-4 col-md-offset-3 col-lg-offset-2"><header class="hn-docs-header page-header"><h1>Harp Multiclass Logistic Regression with Stochastic Gradient Descent</h1><div class="hn-docs-description"></div></header><article class="hn-docs-content">

<p><img src="https://dsc-spidal.github.io/harp-test/img/4-3-1.png" width="60%"  ></p>

<p>Multiclass logistic regression (MLR) is a classification method that generalizes logistic regression to multiclass problems, i.e. with more than two possible discrete outcomes. That is, it is a model that is used to predict the probabilities of the different possible outcomes of a categorically distributed dependent variable, given a set of independent variables.</p>

<p>The process of the MLR algorithm is:</p>

<ol>
<li><p>Use the weight <code>W</code> to predict the label of current data point.</p></li>

<li><p>Compare the output and the answer.</p></li>

<li><p>Use SGD to approximate <code>W</code>.</p></li>

<li><p>Repeat step 1 to 3 with each label and their weights.</p></li>
</ol>

<p>Stochastic gradient descent (SGD) is a stochastic approximation of the gradient descent optimization method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minimums or maximums by iteration. As the algorithm sweeps through the training set, it performs the update for each training example. Several passes can be made over the training set until the algorithm converges.</p>

<p>The SGD algorithm can be described as following:</p>

<ol>
<li><p>Randomly assign the weight <code>W</code>.</p></li>

<li><p>Shuffle <code>N</code> data points.</p></li>

<li><p>Go through <code>N</code> data points and do gradient descent.</p></li>

<li><p>Repeat step 2 and 3 <code>K</code> times.</p></li>
</ol>

<p>Definitions:</p>

<ul>
<li><code>N</code> is the number of data points</li>
<li><code>T</code> is the number of labels</li>
<li><code>M</code> is the number of features</li>
<li><code>W</code> is the <code>T*M</code> weight matrix</li>
<li><code>K</code> is the number of iteration</li>
</ul>

<h2 id="parallel-design">PARALLEL DESIGN</h2>

<ul>
<li><p>What are the model? What kind of data structure?</p>

<p>The weight vectors for classes are model. Because an ovr(one-versus-rest) approach is adopted, each weight vector are independent. It has a matrix structure.</p></li>

<li><p>What are the characteristics of the data dependency in model update computation, can updates run concurrently?</p>

<p>Model update computation here is the SGD update, in which for each data point it should update the model directly. Because of the ovr strategy, each row in the model matrix are independent, and can be updated in parallel.</p></li>

<li><p>which kind of parallelism scheme is suitable, data parallelism or model parallelism?</p>

<p>Data parallelism can be used, i.e., calculating different data points in parallel.</p>

<p>Because the updates can run concurrently, model parallelism is a nature solution.  Each node get one partition of the model, which updates in parallel. And furthermore, thread level parallelism can also follows this model parallelism pattern, that each thread take a subset of partition and update in parallel independently.</p></li>

<li><p>which collective communication operations is suitable to synchronize model?</p>

<p>DynamicScheduler can be used for thread-level parallelism, and Rotate can be used in the inter-node model synchronization.</p></li>
</ul>

<h2 id="dataflow">DATAFLOW</h2>

<p><img src="https://dsc-spidal.github.io/harp-test/img/4-3-2.png" alt="dataflow" /></p>

<h2 id="step-0-data-preprocessing">Step 0 &mdash; Data preprocessing</h2>

<p>Harp MLR will use the data in the vector format. Each vector in a file represented by the format <code>&lt;did&gt; [&lt;fid&gt;:&lt;weight&gt;]</code>:</p>

<ul>
<li><code>&lt;did&gt;</code> is an unique document id</li>
<li><code>&lt;fid&gt;</code> is a positive feature id</li>
<li><code>&lt;weight&gt;</code> is the number feature value within document weight</li>
</ul>

<p>After preprocessing, push the data set into HDFS by the following commands.</p>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>hdfs dfs -mkdir /input
hdfs dfs -put input_data/* /input
</code></pre></div>

<h2 id="step-1-initialize">Step 1 &mdash; Initialize</h2>

<p>For Harp MLR, we will use dynamic scheduling as mentioned above. Before we set up the dynamic scheduler, we need to initialize the weight matrix <code>W</code>, which will be partitioned into <code>T</code> parts representing to <code>T</code> labels which means that each label belongs to one partition and is treated as an independent task.</p>
<div class="highlight"><pre><code class="language-Java" data-lang="Java"><span></span><span class="kd">private</span> <span class="kt">void</span> <span class="nf">initTable</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">wTable</span> <span class="o">=</span> <span class="k">new</span> <span class="n">Table</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="k">new</span> <span class="n">DoubleArrPlus</span><span class="o">());</span>
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">topics</span><span class="o">.</span><span class="na">size</span><span class="o">();</span> <span class="n">i</span><span class="o">++)</span>
        <span class="n">wTable</span><span class="o">.</span><span class="na">addPartition</span><span class="o">(</span><span class="k">new</span> <span class="n">Partition</span><span class="o">(</span><span class="n">i</span><span class="o">,</span> <span class="n">DoubleArray</span><span class="o">.</span><span class="na">create</span><span class="o">(</span><span class="n">TERM</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="kc">false</span><span class="o">)));</span>
<span class="o">}</span>
</code></pre></div>

<p>After that we can initialize the dynamic scheduler. Each thread will be treated as a worker and be added into the scheduler. The only thing that needs to be done is that tasks has to be submitted during the computation.</p>
<div class="highlight"><pre><code class="language-Java" data-lang="Java"><span></span><span class="kd">private</span> <span class="kt">void</span> <span class="nf">initThread</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">GDthread</span> <span class="o">=</span> <span class="k">new</span> <span class="n">LinkedList</span><span class="o">&lt;&gt;();</span>
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numThread</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span>
        <span class="n">GDthread</span><span class="o">.</span><span class="na">add</span><span class="o">(</span><span class="k">new</span> <span class="n">GDtask</span><span class="o">(</span><span class="n">alpha</span><span class="o">,</span> <span class="n">data</span><span class="o">,</span> <span class="n">topics</span><span class="o">,</span> <span class="n">qrels</span><span class="o">));</span>
    <span class="n">GDsch</span> <span class="o">=</span> <span class="k">new</span> <span class="n">DynamicScheduler</span><span class="o">&lt;&gt;(</span><span class="n">GDthread</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div>

<h2 id="step-2-mapper-communication">Step 2 &mdash; Mapper communication</h2>

<p>In this main process, we use <code>regroup</code> to distribute the partitions to the workers first. The workers will get almost the same number of partitions. Then we start the scheduler. For each time we submit one partition to each thread in the scheduler and the threads will all use SGD to approximate <code>W</code> with each label. After the workers finish once with their own partitions, we will use <code>rotate</code> operation to swap the partitions among the workers. When finishing the all process, each worker should use its own data training the whole partition <code>K</code> times, of which <code>K</code> is the number of iteration. <code>allgather</code> operation collects all partitions in each worker, combines the partitions, and shares the outcome with all workers. Finally, the Master worker outputs the weight matrix <code>W</code>.</p>
<div class="highlight"><pre><code class="language-Java" data-lang="Java"><span></span><span class="kd">protected</span> <span class="kt">void</span> <span class="nf">mapCollective</span><span class="o">(</span><span class="n">KeyValReader</span> <span class="n">reader</span><span class="o">,</span> <span class="n">Context</span> <span class="n">context</span><span class="o">)</span> <span class="kd">throws</span> <span class="n">IOException</span><span class="o">,</span> <span class="n">InterruptedException</span> <span class="o">{</span>
    <span class="n">LoadAll</span><span class="o">(</span><span class="n">reader</span><span class="o">);</span>
    <span class="n">initTable</span><span class="o">();</span>
    <span class="n">initThread</span><span class="o">();</span>

    <span class="n">regroup</span><span class="o">(</span><span class="s">&quot;MLR&quot;</span><span class="o">,</span> <span class="s">&quot;regroup_wTable&quot;</span><span class="o">,</span> <span class="n">wTable</span><span class="o">,</span> <span class="k">new</span> <span class="n">Partitioner</span><span class="o">(</span><span class="n">getNumWorkers</span><span class="o">()));</span>

    <span class="n">GDsch</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>        
    <span class="k">for</span> <span class="o">(</span><span class="kt">int</span> <span class="n">iter</span> <span class="o">=</span> <span class="mi">0</span><span class="o">;</span> <span class="n">iter</span> <span class="o">&lt;</span> <span class="n">ITER</span> <span class="o">*</span> <span class="n">numMapTask</span><span class="o">;</span> <span class="n">iter</span><span class="o">++)</span> <span class="o">{</span>
        <span class="k">for</span> <span class="o">(</span><span class="n">Partition</span> <span class="n">par</span> <span class="o">:</span> <span class="n">wTable</span><span class="o">.</span><span class="na">getPartitions</span><span class="o">())</span>
            <span class="n">GDsch</span><span class="o">.</span><span class="na">submit</span><span class="o">(</span><span class="n">par</span><span class="o">);</span>
        <span class="k">while</span> <span class="o">(</span><span class="n">GDsch</span><span class="o">.</span><span class="na">hasOutput</span><span class="o">())</span>
            <span class="n">GDsch</span><span class="o">.</span><span class="na">waitForOutput</span><span class="o">();</span>
            
        <span class="n">rotate</span><span class="o">(</span><span class="s">&quot;MLR&quot;</span><span class="o">,</span> <span class="s">&quot;rotate_&quot;</span> <span class="o">+</span> <span class="n">iter</span><span class="o">,</span> <span class="n">wTable</span><span class="o">,</span> <span class="kc">null</span><span class="o">);</span>

        <span class="n">context</span><span class="o">.</span><span class="na">progress</span><span class="o">();</span>
    <span class="o">}</span>
    <span class="n">GDsch</span><span class="o">.</span><span class="na">stop</span><span class="o">();</span>
        
    <span class="n">allgather</span><span class="o">(</span><span class="s">&quot;MLR&quot;</span><span class="o">,</span> <span class="s">&quot;allgather_wTable&quot;</span><span class="o">,</span> <span class="n">wTable</span><span class="o">);</span>

    <span class="k">if</span> <span class="o">(</span><span class="n">isMaster</span><span class="o">())</span>
        <span class="n">Util</span><span class="o">.</span><span class="na">outputData</span><span class="o">(</span><span class="n">outputPath</span><span class="o">,</span> <span class="n">topics</span><span class="o">,</span> <span class="n">wTable</span><span class="o">,</span> <span class="n">conf</span><span class="o">);</span>
    <span class="n">wTable</span><span class="o">.</span><span class="na">release</span><span class="o">();</span>
<span class="o">}</span>
</code></pre></div>

<h2 id="usage">USAGE</h2>
<div class="highlight"><pre><code class="language-bash" data-lang="bash"><span></span>$ hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.mlr.MLRMapCollective <span class="o">[</span>alpha<span class="o">]</span> <span class="o">[</span>number of iteration<span class="o">]</span> <span class="o">[</span>number of features<span class="o">]</span> <span class="o">[</span>number of workers<span class="o">]</span> <span class="o">[</span>number of threads<span class="o">]</span> <span class="o">[</span>topic file path<span class="o">]</span> <span class="o">[</span>qrel file path<span class="o">]</span> <span class="o">[</span>input path in HDFS<span class="o">]</span> <span class="o">[</span>output path in HDFS<span class="o">]</span>
<span class="c1">#e.g. hadoop jar harp-tutorial-app-1.0-SNAPSHOT.jar edu.iu.mlr.MLRMapCollective 1.0 100 47236 2 16 /rcv1v2/rcv1.topics.txt /rcv1v2/rcv1-v2.topics.qrels /input /output</span>
</code></pre></div>

<p>The output should be the weight matrix <code>W</code>.</p>
</article></section></div></div></div><script src="https://code.jquery.com/jquery-2.2.1.min.js"></script><script src="https://dsc-spidal.github.io/harp-test/js/app.min.js"></script></body></html>
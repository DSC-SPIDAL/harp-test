\section{Introduction}
Machine learning is one of the most powerful areas we are interested in. It helps to solve numerous problems and are widely used in both academy and industry. There are a certain of machine learning algorithms that probably most of us are all familiar with, such as K-means, support vector machine\cite{suykens1999least}, neural networks\cite{haykin2004comprehensive} and so on. Each time we implement these algorithms, there is always the question to ask, how can we do better? Even with the same algorithm, the runtime can varies a lot depending on the method of implementation. Distributed systems\cite{tanenbaum2007distributed} pop up in late 20th century and thus give us a great way to answer the question.

Distributed system is a model whose components located on networked computers that communicate and coordinate their actions by passing messages. From the lecture in class, we have a more easy-understanding concept of it: a distributed system is a collection of independent computers that appears to its users as a single coherent system. With a distributed system, we can solve computational problem by ease, since we can allocate the jobs to multiple machines, and thus largely reduces the runtime. Distributed computing is also great for iterative problems such as PageRank\cite{page1999pagerank}. 

MapReduce\cite{dean2008mapreduce} is a programming model and an associated implementation to process and generate large data sets. We have a users specify "map" function and a "reduce" function. The map function processes a key/value pair to generate a set of intermediate key/value pairs, and the reduce function that merges all intermediate values associated with the same intermediate key. We write programs in this functional style so they are automatically parallelized and can be executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning, scheduling, handling failures, and managing communication. This greatly benefits programmer who has little experience with parallel and distributed systems to easily utilize the resources of a large distributed system.

The High Performance Computing(HPC)\cite{dowd1993high} and Grid Computing\cite{foster2003grid} are for large-scale data processing by using Application Program Interfaces(APIs) as Message Passing Interface(MPI)\cite{gropp1996high}. MapReduce helps to accelerate data access by collocating the data with the compute node. Therefore we have MapReduce implementations to control data locality. However, MapReduce only operates at the higher level. MPI gives control to the programmer and handles the mechanics of the data flow.

When dealing with large scale data set, we can choose to divide the work into many tasks, and give the task to one or more machines. When the computation is simple and uses little time to get the job done, we can only run the work on a single machine. When the computation is large and involving complex algorithm or iterations, the work is "cooperated" by the coherent system of multiple machines. Hence we have the necessarily of communication among all machines, for they need to know the current state of the on-doing job and receive commands from the master. Each single machine communicate with others by message passing.

Message passing sends a message to a process relying on the process as well as the supporting infrastructure to select and invoke the actual code to run. When we analyze the runtime of an algorithm, we also need to take into consideration of the communication cost. There are typically 2 communications of one node in a communication round: receiving messages from the neighbors in the parallel, sending new messages to its neighbors after the local computation. Let $N$ be the number of rounds, then the total times of communication of a node is $2N$. This can be very large when the number of nodes is considerable. Here we choose another way of communication called broadcast. It sends the updated message from the master node to all slaver nodes at the beginning of each round, and receive messages from all nodes at the end of a round of computation. This is much more convenient and saves I/Os.

With all the features above, there are many excellent framework for distributed applications. One of the most promising one is Apache Hadoop\cite{white2012hadoop}, which is also our focus of our course. Apache Hadoop is a software framework for distributed storage and distributed processing of very large data sets on computer clusters built from commodity hardware. The reason we love it correspond with its reliability, high efficiency and fault tolerance. Also, it is open-source and can be build on commercial machines, thus make it easy-use for both personal users and companies. It is designed for high-throughput applications, so it is very suitable for distributed computation.

Harp\cite{zhang2015harp} is a plugin to Hadoop. It provides collective communication library as well as associated data abstraction. By plugging Harp into Hadoop, we can convert MapReduce model into Map-Collective model and enable efficient in-memory communication. In this way we have parallel processes cooperate through collective communication for efficient data processing. What's more, Harp is neither a replication of MPI nor an attempt to transplant MPI into the Hadoop system.

It is highly recommended when we want to run iterative algorithms because of its high speed. For most of the algorithms, we can implement them simply on Hadoop. However, the reason we use Harp is that Harp's communication is based on memory while Hadoop is based on disk. Too many I/Os will largely reduce the performance. This results in less of runtime. Take PageRank as an example, Harp is much more faster than Hadoop on different experiment settings. We also has result from experiment on Pig showing Harp provides fast data caching and customized communication patterns among iterations\cite{wu2014integrating}. Since Hadoop and Apache open-source stacks are designed as the mainstream tools for handling big data problems, the development of a Hadoop plug-in to support Iterative MapReduce can lead to best performance.

Among of certain algorithms in machine learning, here we choose support vector machines for implementation on Harp. Support vector machines(SVM) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a training data set, an SVM training algorithm divide the datas into several parts(mostly two) which each projects to a class it belongs. It can not only solve linear problems in machine learning but also polynomial ones. An SVM model represents every data point as a point in space, and maps them onto a plate of a new dimension. After one or several mappings, the points can finally be divided by a clear gap that is as wide as possible. For each mapping, SVM uses a mapping function. By reversing all the functions, we can go back from the single line(the dividing gap) to a curved surface that divides the original data set.\\

\textbf{Related Work}\\
When training a support vector machine, we need to find out the bound constraints and a linear equality constraint. This quadratic optimization problem may goes into many issues we need to consider in designing an SVM model\cite{joachims1999making}. When the training set is large and the learning tasks are considerable, time and memory are big limitations.  At worst time, SVMs scale badly with large data size due to the quadratic optimization algorithm and the kernel transformation\cite{meyer2015support}. SVM can also be very sensitive to the chosen parameter and noise. Making the correct choice of kernel parameters is crucial for obtaining good results, which also means that an extensive search must be conducted on the parameter space before results can be trusted, and this often complicates the task. The current implementation is only optimized for the radial basis function kernel, which might still be suboptimal for the data.

Harp is good for SVM implementation but it needs to overcome the huge amount of memory used and the computation complexity. Cascade SVM\cite{graf2004parallel} was proposed right for this challenge. Dataset is split into parts in feature space in this method. For each sub-dataset, the non-support vectors are filtered and only the support vectors are transmitted to the next round. Therefore, the margin optimization process need only the combined sub-dataset and can thus find out the support vectors. Another method proposed for parallel SVM training is that, for each subset of a dataset, it is trained with SVM and then the classifiers are combined into a final single classifier function. We also see propose in distributed SVM based on strongly connected network\cite{lu2008distributed}. In this way, a dataset is split into equal parts for each computer in a network. Then the support vectors are exchanged among the related computers. Upon the use of subSVM, there is an interesting paper develops a parallel SVM model based on MapReduce. As well as methods used before, training samples are divided into subsections, but each subsection is trained with a SVM model and libSVM\cite{chang2011libsvm} is used to train each subSVM. Then the non-support vectors will be filtered with subSVMs, and the support vectors of each subSVM are taken as the input of next layer subSVM. Finally, the global SVM model can be obtained through iterations and the output is obtained.

